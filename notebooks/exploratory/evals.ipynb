{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import subprocess\n",
    "\n",
    "######################\n",
    "## TESTS\n",
    "######################\n",
    "t1 = [0,0,0,0]\n",
    "t2 = [0,1,1,0]\n",
    "t3 = [0,6,0]\n",
    "t4 = [1,0,-1]\n",
    "t5 = [10,10,-10,-10]\n",
    "\n",
    "test_value = [0,2,6,0,0]\n",
    "tests = [t1,t2,t3,t4,t5]\n",
    "for test in tests:\n",
    "    np.testing.assert_allclose(np.trapz(test),test_value[tests.index(test)])\n",
    "\n",
    "######################\n",
    "## MAIN\n",
    "######################\n",
    "\n",
    "'''\n",
    "If we have a pgn file with games which have analysis (e.g some lichess dbs) run evals.cql to remove it to just this\n",
    "o/w do some sf analysis\n",
    "'''\n",
    "\n",
    "def find_analysed_games():\n",
    "    cql_command = cql_dir+' -i '+db_dir+' -o '+pgn_file+' -matchcount 2 100 '+file_dir\n",
    "    subprocess.run(cql_command,shell=True)\n",
    "    return 0\n",
    "\n",
    "with open(sys.argv[1]) as f:\n",
    "    content = str(f.readlines())\n",
    "    content = content.split('[Event')\n",
    "content = content[1:]\n",
    "result_array = []\n",
    "tc_array = []\n",
    "eval_array = np.zeros((len(content),400))\n",
    "game_len_max = 0\n",
    "\n",
    "# extract time controls\n",
    "# extract game evals\n",
    "# extract result\n",
    "# extract max length of game\n",
    "for item in content:\n",
    "    game_index = content.index(item)\n",
    "    game_evals = re.findall(r'eval\\s(-?\\d{1,2}.\\d{1,2}|#-?\\d{1,2})', item)\n",
    "    timecontrol = re.findall(r'(Rated\\s\\w{1,9}\\sgame|TimeControl\\s\".{1,9}\")', item)\n",
    "    result = re.findall(r'Result\\s\".{3,7}\"', item)\n",
    "    tc_array.append(timecontrol[0].split()[1])\n",
    "    result_array.append(result[0][7:])\n",
    "    if len(game_evals) > game_len_max:\n",
    "        game_len_max = len(game_evals)\n",
    "    try:\n",
    "        eval_array[game_index, 0:len(game_evals)] = game_evals\n",
    "    except ValueError:\n",
    "        for value in game_evals:\n",
    "            if '#' in value:\n",
    "                game_evals[game_evals.index(value)] = np.copysign(200,int(value[1:]))\n",
    "        eval_array[game_index, 0:len(game_evals)] = game_evals\n",
    "\n",
    "eval_array = eval_array[:,0:game_len_max]\n",
    "\n",
    "def auc(eval_array):\n",
    "    auc_score = []\n",
    "    avg_auc = []\n",
    "    for game in range(np.shape(eval_array)[0]):\n",
    "        auc_score.append(np.trapz(eval_array[game,:]))\n",
    "        avg_auc.append(np.trapz(eval_array[game,:])/len(eval_array[game,:]))\n",
    "    return auc_score, avg_auc\n",
    "\n",
    "game = eval_array[400,:]\n",
    "plt.plot(game[0:len(game) - np.equal(game,0)[::-1].argmin()])\n",
    "plt.show()\n",
    "# filter game by time control\n",
    "\n",
    "'''\n",
    "blitz = np.array([tc == 'Bullet' for tc in tc_array])\n",
    "print((len(blitz),len(auc_score),len(result_array)))\n",
    "for idx in range(len(blitz)):\n",
    "    if blitz[idx] == 0:\n",
    "        auc_score[idx] = 0\n",
    "        result_array[idx] = 0\n",
    "auc_score = [i for i in auc_score if i != 0]\n",
    "result_array = [i for i in result_array if i != 0]\n",
    "plt.scatter(auc_score, result_array)\n",
    "plt.show()\n",
    "'''\n",
    "#######################\n",
    "## CORR\n",
    "#######################\n",
    "\n",
    "codes = {'1-0':1,'0-1':0,'1/2-1/2':0.5}\n",
    "for result in result_array:\n",
    "    if result == '\"1-0\"':\n",
    "        result_array[result_array.index(result)] = 1\n",
    "    elif result == '\"0-1\"':\n",
    "        result_array[result_array.index(result)] = -1\n",
    "    elif result == '\"1/2-1/2\"':\n",
    "        result_array[result_array.index(result)] = 0\n",
    "print(np.mean(result_array))\n",
    "print(np.corrcoef(auc_score, result_array))\n",
    "\n",
    "X = np.array(avg_auc).reshape(-1,1)\n",
    "Y = np.array(result_array)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.4,random_state=100)\n",
    "\n",
    "logreg= LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred=logreg.predict(X_test)\n",
    "#print(X_test) #test dataset\n",
    "#print(y_pred) #predicted values\n",
    "\n",
    "print('Accuracy: ',metrics.accuracy_score(y_test, y_pred))\n",
    "print('Recall: ',metrics.recall_score(y_test, y_pred, zero_division=1,average='micro'))\n",
    "print('Precision: ',metrics.precision_score(y_test, y_pred, zero_division=1,average='micro'))\n",
    "print('CL Report: ',metrics.classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
